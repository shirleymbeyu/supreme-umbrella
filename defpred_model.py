# -*- coding: utf-8 -*-
"""DefPred_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OC5rHmom_gpAkCFVjg_RAOBFIjtsUC-W

#Set up
"""

!pip install --upgrade fastcore -q
!pip install --upgrade fastai -q

from fastai.vision.all import * # Needs latest version, and sometimes a restart of the runtime after the pip installs

!pip install xgboost==0.90

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
np.random.seed(123)
warnings.filterwarnings('ignore')
# %matplotlib inline

ts = pd.read_csv('defpred_ts.csv')
ts.head()

ts = ts.drop(['Unnamed: 0', 'invoice_date'], axis = 1)

ts.shape

ts[:2]

tr = pd.read_csv('defpred_tr.csv')
tr.head()

tr = tr.drop(['Unnamed: 0', 'invoice_date', 'creation_date'], axis = 1)

tr.shape

tr[:2]

"""# Data Preparation"""

#import preprocessing module
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler

# Convert target label to numerical Data
le = LabelEncoder()
tr["target"]= le.fit_transform(tr["target"])

sns.countplot('target', data = tr);

X_train = tr.drop(['target'], axis=1)
y_train = tr['target']

"""We have created a simple preprocessing function to:

Handle conversion of data types

Convert categorical features to numerical features by using One-hot Encoder and Label Encoder

Perform feature scaling.

The processing function will be used for both train and test independent variables.
"""

# function to preprocess our data from train models

def preprocessing_data(data):

    # Convert the following numerical labels from interger to float
    float_array = data[["disrict", "client_catg", "region", "tarif_type","counter_statue","reading_remarque", "counter_coefficient", "old_index", "new_index" ,"months_number"]].values.astype(float)
    
    # Label Encoder conversion
    data["counter_type"] = le.fit_transform(data["counter_type"])
    
    
    # drop client id column
    data = data.drop(["client_id"], axis=1)
    
    # scale our data into range of 0 and 1
    scaler = MinMaxScaler(feature_range=(0, 1))
    data = scaler.fit_transform(data)
    
    return data

X_train[:2]

# preprocess the train data 
processed_train_data = preprocessing_data(X_train)

# preprocess the test data 
processed_test_data = preprocessing_data(ts)

# the first train row
print(processed_train_data[:1])

# shape of the processed train set
print(processed_train_data.shape)

"""# Modelling"""

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(processed_train_data, y_train, test_size = 0.7, random_state=42, stratify = y_train)

#import classifier algorithm here
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import ExtraTreesClassifier
from xgboost import XGBClassifier


# create models
#lg_model = LogisticRegression()
#rf_model = RandomForestClassifier()
kn_model = KNeighborsClassifier()
#et_model = ExtraTreesClassifier()
#xg_model = XGBClassifier()


#training the models
#lg_model.fit(X_train,y_train)
#rf_model.fit(X_train,y_train)
kn_model.fit(X_train,y_train)
#et_model.fit(X_train,y_train)
#xg_model.fit(X_train,y_train)

#making predictions
#lg_y_pred = lg_model.predict(X_val)
#rf_y_pred = rf_model.predict(X_val)
kn_y_pred = kn_model.predict(X_val)
#et_y_pred = et_model.predict(X_val)
#xg_y_pred = xg_model.predict(X_val)

"""## Model Evaluation
The metric used for this challenge is AUC.
"""

# import evaluation metrics
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import roc_curve, roc_auc_score

# evaluate the model
# roc_auc_score
#print("Logistic Regression classifier: ", roc_auc_score(y_val, lg_y_pred))
#print("Random Forest classifier: ", roc_auc_score(y_val, rf_y_pred))
print("KNeighbors Classifier: ", roc_auc_score(y_val, kn_y_pred))
#print("Extra Tree classifier: ", roc_auc_score(y_val, et_y_pred))
#print("XGB classifier: ", roc_auc_score(y_val, xg_y_pred))

from sklearn.metrics import plot_confusion_matrix

# Get confusion matrix for Gradient Boosting Classifier 
plot_confusion_matrix(kn_model,X_val, y_val,normalize='true')

"""## Increasing Model Perfomance
 For xgb
"""

from sklearn.model_selection import GridSearchCV

# Optimize model paramaters 
param_grid = {'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1],
        'subsample': [0.6, 0.8, 1.0],
        'max_depth': [3,5]
        }
my_xg_model = GridSearchCV(xg_model, param_grid,n_jobs=-1,verbose=2,cv=5)
my_xg_model.fit(X_train, y_train)
print(my_xg_model.best_params_)

from sklearn.metrics import confusion_matrix, accuracy_score


# fit by setting best parameters and Evaluate model
xgb_model = XGBClassifier(min_child_weight=1, gamma=1, subsample=1.0, max_depth=5)

xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_val)

# Get error rate
print("New XGB classifier: ", roc_auc_score(y_val, y_pred))

"""## Predicting"""

# Get the predicted result for the test Data with the mode of choice
test_target = kn_model.predict(processed_test_data)

test_target.shape

"""# Submission"""

ss = pd.read_csv('SampleSubmission.csv')
ss.head()

# crete submission DataFrame
submission = pd.DataFrame({"client_id": ts["client_id"],"target": test_target})

submission

submission.duplicated().sum()

n = submission.drop_duplicates()

n["target"].astype(float)

n

n.to_csv('kn2.csv', index= False)

"""#"""